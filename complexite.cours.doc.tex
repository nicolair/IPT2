\input{ipt.tex}

%En tete et pied de page
%\lhead{Cours IPT}
\chead{Cours 5 : Introduction à la complexité algorithmique - le 20/11/2019}
\begin{document}
\section{\'Evaluations de complexité}
\subsection{Types de complexité}
\'Evaluer une complexité c'est établir qu'une certaine fonction d'un entier naturel $n$ est \emph{dominée} (au sens mathématique) par une suite usuelle.\newline
La liste suivante présente les  complexités usuelles
\begin{itemize}
  \item $O(1)$: complexité constante. La suite est majorée. 
  \item $O(\ln(n))$: complexité sous-linéaire ou logarithmique.
  \item $O(n)$: complexité linéaire.
  \item $O(n\ln(n))$: complexité quasi-linéaire.
  \item $O(n^2)$: complexité quadratique.
  \item $O(n^k)$: complexité polynomiale.
  \item $O(C^n)$: complexité exponentielle.
\end{itemize}
\subsection{Complexité: de quoi ? en fonction de quoi?}
On parle ici de la complexité d'un algorithme. Quelles sont les fonctions permettant de déterminer la complexité d'un algorithme donné?\index{complexité d'un algorithme} 
\subsubsection{Nombres d'instructions.}
Définissons d'abord les valeurs des fonctions à comparer aux suites usuelles.\newline
Il s'agit d'évaluer les ressources utilisées par un algorithme c'est à dire les instructions élémentaires exécutées par l'algorithme (sans tenir compte de la machine et du langage avec lequel l'algorithme est implémenté).\newline
Les instructions élémentaires sont de divers types: lecture-écriture en mémoire, calcul avec des entiers, calcul avec des flottants.
Dans la pratique, on se limite à considérer un type particulier d'instructions et à évaluer le nombre des instructions de ce type réellement exécutées.\newline
Dans le cas des calculs (entiers ou flottants), on parle de \emph{complexité en temps}\index{complexité en temps}. En multipliant le nombre d'opérations par le nombre maximum d'opérations par seconde pour une machine donnée, on dispose d'une évaluation du temps de l'exécution. On ne considère pas forcément toutes les instructions et ce choix n'est pas universel. Souvent, on ne compte pas les instructions du type incrémenter un compteur entier ou le tester contre un entier.\newline
Dans le cas des instructions de lecture-écriture, comme celles ci sont très rapides, on considère la taille de la mémoire nécessaire à ces instructions plutôt que le nombre de ces instructions. On parle alors de \emph{complexité en espace}\index{complexité en espace}. Pour une implémentation particulière, si la complexité en espace dépasse la mémoire vive allouée au processus deux situations peuvent se produire. Si l'OS n'est pas efficace le processus de l'algorithme va compromettre le fonctionnement de la machine elle même. Si l'OS est efficace, les instructions de lecture-écriture vont utiliser un mode de secours et être très lentes.

\subsubsection{Taille des données.}
Quel est l'argument des fonctions donnant les nombres d'opérations? Pour les complexités usuelles, que représente le $n$?\newline
L'exécution de l'algorithme se fait à partir d'une donnée constituée d'un jeu de données numériques (booléennes, entières, flottantes). \emph{En général, la donnée n'est pas un nombre en elle même}.\newline
On doit préciser un entier naturel que l'on appelle la \emph{taille} de la donnée pour l'algorithme considéré. Il existe plusieurs données de même taille et deux données de même taille ne conduisent pas à l'exécution du même nombre d'instructions. On ne cherche donc pas une expression exacte du nombre d'instructions mais plutôt une majoration $c(n)$ valable pour toutes les données de taille $n$. C'est cette fonction que l'on cherche à dominer par les suites usuelles. \newline
On peut aussi définir la \emph{complexité dans le meilleur des cas}\index{complexité dans le meilleur des cas},  la \emph{complexité dans le pire des cas}\index{complexité dans le pire des cas} et la \emph{complexité moyenne}\index{complexité moyenne} pour une taille de donnée $n$ fixée.\newline
Pour la complexité en moyenne, on introduit une probabilité sur l'ensemble des données de taille $n$, le nombre d'instructions est une variable aléatoire sur cet espace probabilisé et la complexité moyenne est l'espérance de cette variable aléatoire.

\subsection{Estimations en temps}
On mesure la rapidité d'un processeur au nombre d'instructions qu'il peut exécuter par seconde. \index{flops}L'unité est le \emph{flops} (floating operations per second).\newline
En 2013 un ordinateur personnel peut disposer d'une puissance de 200 gigaFLOPS = $200\times 10^{9}$ FLOPS. Le tableau suivant présente une estimation , en fonction de la complexité, des temps de calcul sur un tel ordinateur pour des algorithmes avec une donnée de taille $10^{6}$.
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|l|l|l|l|l|} \hline
log. & lin. & quasi-lin. & quad. & pol. (3) & pol. (4)\\ \hline
69 ns & 5 $\mu$s & 69 $\mu$s & 5 s & 57 jours & 158548 ans \\ \hline
\end{tabular}
\end{center}


\section{Exemples}
\subsection{Enchaînement d'algorithmes}
Considérons un algorithme qui s'exécute avec des données de taille $n$. S'il est constitué de deux algorithmes successifs: un pré-traitement qui est de complexité constante et le traitement proprement dit qui est de complexité linéaire, l'algorithme complet est aussi de complexité linéaire.

\subsection{Complexité logarithmique d'une recherche dichotomique}
\begin{algorithm}
  \Donnees{\;
    $L \leftarrow$ une liste (indexée à partir de 0) croissante (pas forcément strictement) de $n$ objets comparables pour une relation $\leq$.\;
    $v \leftarrow$ un objet vérifiant $L[0]\leq v < L[n-1]$\;
  }
  \Comment{initialisation}
  $i\leftarrow 0$\;
  $j\leftarrow n-1$\;
  \Comment{$k$ désignera un entier strictement entre $i$ et $j$}
  \Tq{ $j-i > 1$}{
    $k\leftarrow$ partie entière de $\frac{i+j}{2}$ \;
    \eSi{L[k] $\leq$ v}{
      $i\leftarrow k$ \;
    }{
      $j\leftarrow k$
    }
  }
  \Comment{$i$ désigne l'indice cherché.}
  \caption{Encadrement par dichotomie dans une liste}
  \label{complexite_1}
\end{algorithm}
Dans l'algorithme \ref{complexite_1}, on compte seulement les instructions d'affectation. La taille des données est la longueur $n$ des listes.\newline
L'initialisation est un pré-traitement de complexité constante.\newline
L'expression $j-i$ est une fonction de terminaison de la boucle car elle est à valeur dans $\N$ et divisée par deux à chaque itération. La boucle se termine donc après un nombre d'itérations noté $p$. Alors
\begin{displaymath}
  \frac{n-1}{2^{p-1}} > 1
\Rightarrow \ln(n-1) > (p-1) \ln(2)  
\Rightarrow p < 1 + \frac{\ln(n-1)}{\ln(2)}
\end{displaymath}
Chaque itération comprend deux affectations et 
\begin{displaymath}
  \left( 2 + \frac{2*\ln(n-1)}{\ln(2)}\right)_{n\geq 3} 
\end{displaymath}
est dominée par $(\ln n)_{n\in \N^*}$. L'initialisation étant de complexité constante, la complexité est ici logarithmique.

\subsection{Formation de la liste des diviseurs d'un nombre}
\begin{algorithm}
  $m\leftarrow$ un entier représenté en binaire avec $p$ bits\;
  $Div \leftarrow$ une liste contenant $1,-1,m,-m$\;
  $k \leftarrow 2$\;
  \Tq{ $k^2 < |m|$}{
    \Si{$k$ divise $m$}{
      placer $k, -k, m/k, -m/k$ à la fin de $Div$\;
    }
    incrémenter $k$\;
  }
  \caption{liste de diviseurs}
  \label{complexite_2}
\end{algorithm}
Dans cet exemple, les données sont des nombres, on évalue le nombre d'affectations. Le nombre d'affectations pour une donnée $m$ est en $O(\sqrt{m})$.\newline
Attention dans l'expression précédente, $m$ est la donnée elle même et non la taille de la donnée. On peut décider que la taille de la donnée est $n=2^p$. Le pire cas est évidemment le plus grand nombre possible pour une taille fixée. La complexité est donc en $O(\sqrt{n})$.

\subsection{Complexité optimale d'un tri}\label{opti}
On considère ici le \emph{problème} du tri croissant d'une liste d'objets comparables et deux à deux distincts.\newline
Un \emph{algorithme de tri} forme une liste strictement croissante à partir de la donnée d'une liste de $n$ nombres deux à deux distincts\footnote{d'après \href{http://en.wikipedia.org/wiki/Comparison_sort}{http://en.wikipedia.org/wiki/Comparison\_sort}}.
On peut obtenir un résultat théorique sur la complexité d'un tel algorithme considérée comme le nombre de comparaisons en fonction de la longueur $n$ de la liste prise pour taille des données.\newline
Il existe $n!$ manières de permuter les valeurs et une seule conduit à une liste strictement croissante. Un algorithme de tri doit obtenir assez d'informations à partir des comparaisons qu'il effectue pour déterminer cette permutation.\newline
Si l'algorithme termine toujours après $c(n)$ comparaisons, il a pu distinguer $2^{c(n)}$ cas et ces cas doivent recouvrir toutes les permutations possibles soit
\begin{displaymath}
  n! \leq 2^{c(n)}\Rightarrow c(n)\geq \frac{1}{\ln 2} \ln(n!)
\end{displaymath}
Or $\ln(n!)\sim n\ln(n)$ donc $n\ln n \in O(c(n))$. Il n'existe donc pas d'algorithme de tri dont la complexité dans le pire des cas soit meilleure que quasi linéaire. Il existe effectivement des algorithmes de cette complexité qui est alors optimale.

\subsection{Sommes, tranches et dominos}
La donnée est une liste $[a_0,a_1,\cdots,a_{n-1}]$ de $n$ nombres indexée à partir de $0$. Pour un entier $m$ tel que $0\leq m \leq n$, une \emph{tranche} de longueur $m$ est une liste de $m$ valeurs consécutives de la liste. Il existe donc $n-m$ tranches, on souhaite calculer les sommes de ces tranches et évaluer le nombre d'opérations (addition ou soustraction) à exécuter.\newline
Algorithme naïf. On calcule directement les sommes. Chacune est le résultat de $m-1$ additions, on exécute donc $(n-m)(m-1)$ additions.\newline
Algorithme amélioré. On effectue un pré-traitement qui consiste à calculer les sommes partielles
\begin{displaymath}
  A_i = a_0 + a_1 + \cdots + a_i
\end{displaymath}
On doit exécuter $n$ opérations car 
\begin{displaymath}
A_{-1} = 0, A_0 = a_0, A_1 = A_0 + a_1, A_2 = A_1 + a_2,\cdots A_n = A_{n-1} + a_n
\end{displaymath}
Pour calculer la somme d'une tranche, une seule opération est exécutée
\begin{displaymath}
  a_k+\cdots+a_{k+m-1} = A_{k+m-1} - A_{k-1}
\end{displaymath}
Avec cet algorithme amélioré, on effectue seulement $2n-m$ opérations.

\subsection{Prétraitement par tableau de décompte}
\subsubsection{Nombres d'occurrences}
$m$ un entier naturel fixé. Les données sont des tableaux de nombres entre $0$ et $m$. On considère que la taille des données est la longueur $n$ des tableaux et que les instructions à compter sont les assignations et les additions.\newline
Si $A$ est un tel tableau, on appelle \emph{tableau de décompte} de $A$, un tableau $C$ de longueur $m+1$ tel que: 
\begin{displaymath}
  \forall v\in \llbracket 0, m \rrbracket,\; C[v] = \text{nombre d'occurrences de $v$ dans $A$}
\end{displaymath}
Il est intéressant de remarquer que la somme des valeurs du tableau de décompte est la longueur $n$ du tableau donné $A$.\newline
On peut implémenter en Python le calcul d'un tableau de décompte par la fonction suivante:
\lstinputlisting[firstline=8, lastline=12]{complexite.py}
L'algorithme sous-jacent a exécuté $m+1+n$ instructions, il est linéaire en $n$.

\subsubsection{Application au tri}
Plusieurs tableaux différents peuvent avoir le même tableau de décompte mais parmi ceux là, un seul est croissant. On l'obtient par la fonction suivante où $C$ est un tableau dont la somme des valeurs est $n$.
\lstinputlisting[firstline=14, lastline=23]{complexite.py}
Le nombre d'instructions exécutées est $2+2(m+1)+n$, la complexité est donc encore linéaire en $n$.\newline
Il semblerait que l'on ait obtenu un algorithme linéaire de tri en contradiction avec la complexité optimale trouvée en partie \ref{opti}. Pourquoi cela n'est pas le cas?

\subsubsection{\'Echanges de valeurs}
On dispose de deux listes $A$ et $B$ de $n$ nombres entiers naturels dans $\llbracket 0,m\rrbracket$. On veut savoir s'il est possible d'échanger deux valeurs pour que les deux listes aient la même somme. On choisit la longueur des listes comme taille des données et on compte les additions.\newline
Il existe $n^2$ couples de valeurs à échanger. On les considère tous, en calculant les deux sommes à chaque fois et en testant leur égalité. Comme chaque calcul de somme est de complexité linéaire, la complexité dans le pire des cas (pas d'échange satisfaisant) est en $O(n^3)$.\newline
On peut faire mieux commençant par des prétraitements. Notons $s_A$ et $s_B$ les deux sommes. L'échange est possible si et seulement si 
\begin{multline*}
  \exists (i,j)\in \llbracket 0, n-1 \rrbracket \text{ tq } s_A + b_j -a_i = s_B + a_i - b_j\Leftrightarrow s_B - s_A = 2(b_j - a_i)\\
\Leftrightarrow b_j = a_i + \frac{s_B - s_A}{2}
\end{multline*}
On remarque qu'il est nécessaire que $s_B - s_A$ soit pair pour que l'échange soit possible.
Considérons l'algorithme \ref{complexite_3}.
\begin{algorithm}
  \Comment{prétraitement}
  calcul de $S = s_B - s_A$\;
  \Si{$S$ impair}{
    renvoyer \texttt{false}\;
  }
  $S \leftarrow S/2$\;
  calcul de $C=$ tableau de décompte de $B$\;
  \Comment{traitement}
  \PourCh{$i$ entre 0 et $n-1$}{
    $v \leftarrow A[i] + S$\;
    \Si{$C[v]>0$}{
      renvoyer \texttt{true}\;
    }
  }
  renvoyer \texttt{false}\;
  \caption{échange de valeurs}
  \label{complexite_3}
\end{algorithm}
Le prétraitement et le traitement sont linéaires, l'algorithme \ref{complexite_3} est en $O(n)$.

\subsection{Autour du crible d'Eratosthène}
\subsubsection{Liste de nombres premiers}
On peut implémenter en Python le crible d'Eratosthène par la fonction suivante\index{crible d'Eratosthène}.
\lstinputlisting[firstline=25, lastline=36]{complexite.py}
La complexité est en $O(n\ln(\ln(n)))$ mais cela repose sur la propriété
\begin{displaymath}
  \left( \sum_{p\text{ premier }\leq n} \frac{1}{p}\right)_{n\geq 2} \in O(\ln(\ln(n))) 
\end{displaymath}
dont la démonstration est difficile. En revanche, on peut montrer facilement que la complexité est $O(n\ln(n))$.\newline
Pourquoi la boucle \texttt{while} interne démarre-t-elle à $k=p^2$?

\subsubsection{Factorisation}
Pour calculer la décomposition en facteurs premiers d'un entier $n$, on effectue d'abord un prétraitement dérivé du crible d'Eratosthène en calculant un tableau $F$ qui ressemble au précédent. Au lieu de \texttt{False}, on va placer en $F[i]$ le plus petit diviseur premier de $i$ autre que lui même. Dans ces conditions, $i$ est premier si et seulement si $F[i]=0$.
\lstinputlisting[firstline=39, lastline=50]{complexite.py}
Muni de ce tableau $F$ des plus petits facteurs premiers des éléments de $\llbracket 2,n\rrbracket$ on peut former la liste de tous les facteurs premiers d'un $x\in  \llbracket 2,n\rrbracket$.

\begin{algorithm}
  \Donnees{
    $F$ le tableau des plus petits diviseurs premiers,  $x \in \llbracket 2,n \rrbracket$\;
  }
  $facteursPrem \leftarrow$ liste vide \;
  \Tq{$F[x]>0$}{
    placer $F[x]$ à la fin de $facteursPrem$\;
    $x\leftarrow$ quotient de $x$ par $F[x]$
  }
  \Comment{Comme $F[x]=0$, maintenant $x$ est premier}
  placer $x$ à la fin de $facteursPrem$\;
  renvoyer $facteursPrem$ \;
  \caption{liste des facteurs premiers}
  \label{complexite_4}
\end{algorithm}
Le nombre de facteurs premiers de $x$ décroît strictement lors de la boucle, c'est une fonction de terminaison. Si $s$ est ce nombre, comme les nombres premiers sont plus grands que $2$, on a $2^s \leq x$ donc $s\leq \frac{\ln x}{\ln 2}$. La complexité de cette factorisation est donc en $O(\ln(n))$.\newline
Implémentation python de cet algorithme:
\lstinputlisting[firstline=52, lastline=68]{complexite.py}

\end{document}
